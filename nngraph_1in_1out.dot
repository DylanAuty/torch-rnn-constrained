digraph G {
labelloc="t";
label="nngraph_1in_1out";
node [shape = oval]; 
n1[label="Node1\ninput = {torch.CudaTensor[1x50x65]}\lmodule = nn.Identity\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x65]}" tooltip="[./LanguageModelSkipCon.lua]:123_"];
n2[label="Node2\ninput = {torch.CudaTensor[1x50x65]}\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x65]}" tooltip="[[C]]:-1_"];
n3[label="Node3\ninput = {torch.CudaTensor[1x50x512]}\lmodule = nn.Sequential {\l  [input -> (1) -> (2) -> (3) -> output]\l  (1): nn.View(50,-1)\l  (2): nn.Linear(512 -> 65)\l  (3): nn.View(1,50,-1)\l}\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x65]}" tooltip="[./LanguageModelSkipCon.lua]:123_"];
n4[label="Node4\ninput = {torch.CudaTensor[1x50x384],torch.CudaTensor[1x50x128]}\lmapindex = {Node5,Node6}\lmodule = nn.JoinTable\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x512]}" tooltip="[./LanguageModelSkipCon.lua]:84_"];
n5[label="Node5\ninput = {torch.CudaTensor[1x50x256],torch.CudaTensor[1x50x128]}\lmapindex = {Node7,Node8}\lmodule = nn.JoinTable\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x384]}" tooltip="[./LanguageModelSkipCon.lua]:84_"];
n6[label="Node6\ninput = {torch.CudaTensor[1x50x128]}\lmodule = nn.LSTM\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x128]}" tooltip="[./LanguageModelSkipCon.lua]:78_"];
n7[label="Node7\ninput = {torch.CudaTensor[1x50x128],torch.CudaTensor[1x50x128]}\lmapindex = {Node9,Node10}\lmodule = nn.JoinTable\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x256]}" tooltip="[./LanguageModelSkipCon.lua]:84_"];
n8[label="Node8\ninput = {torch.CudaTensor[1x50x128]}\lmodule = nn.LSTM\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x128]}" tooltip="[./LanguageModelSkipCon.lua]:78_"];
n9[label="Node9\ninput = {torch.CudaTensor[1x50x128]}\lmodule = nn.Identity\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x128]}" tooltip="[./LanguageModelSkipCon.lua]:63_"];
n10[label="Node10\ninput = {torch.CudaTensor[1x50x128]}\lmodule = nn.LSTM\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x128]}" tooltip="[./LanguageModelSkipCon.lua]:78_"];
n11[label="Node11\ninput = {torch.CudaTensor[1x50x64]}\lmodule = nn.LSTM\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x128]}" tooltip="[./LanguageModelSkipCon.lua]:57_"];
n12[label="Node12\ninput = {torch.CudaTensor[1x50]}\lmodule = nn.LookupTable\lreverseMap = {}\lgradOutput = {}" tooltip="[./LanguageModelSkipCon.lua]:41_"];
n13[label="Node13\ninput = {torch.CudaTensor[1x50]}\lreverseMap = {}\lgradOutput = {torch.CudaTensor[]}" tooltip="[[C]]:-1_"];
n1 -> n2;
n3 -> n1;
n4 -> n3;
n5 -> n4;
n6 -> n4;
n7 -> n5;
n8 -> n5;
n8 -> n6;
n9 -> n7;
n10 -> n7;
n10 -> n8;
n11 -> n9;
n11 -> n10;
n12 -> n11;
n13 -> n12;
n12[style=filled, fillcolor=red];
}