digraph G {
labelloc="t";
label="nngraph_1in_1out";
node [shape = oval]; 
n1[label="Node1\ninput = {torch.CudaTensor[1x50x65]}\lmodule = nn.Identity\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x65]}" tooltip="[./LanguageModelSkipCon.lua]:123_"];
n2[label="Node2\ninput = {torch.CudaTensor[1x50x65]}\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x65]}" tooltip="[[C]]:-1_"];
n3[label="Node3\ninput = {torch.CudaTensor[1x50x256]}\lmodule = nn.Sequential {\l  [input -> (1) -> (2) -> (3) -> output]\l  (1): nn.View(50,-1)\l  (2): nn.Linear(256 -> 65)\l  (3): nn.View(1,50,-1)\l}\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x65]}" tooltip="[./LanguageModelSkipCon.lua]:123_"];
n4[label="Node4\ninput = {torch.CudaTensor[1x50x128],torch.CudaTensor[1x50x128]}\lmapindex = {Node5,Node6}\lmodule = nn.JoinTable\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x256]}" tooltip="[./LanguageModelSkipCon.lua]:85_"];
n5[label="Node5\ninput = {torch.CudaTensor[1x50x128]}\lmodule = nn.Identity\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x128]}" tooltip="[./LanguageModelSkipCon.lua]:64_"];
n6[label="Node6\ninput = {torch.CudaTensor[1x50x128]}\lmodule = nn.LSTM\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x128]}" tooltip="[./LanguageModelSkipCon.lua]:79_"];
n7[label="Node7\ninput = {torch.CudaTensor[1x50x64]}\lmodule = nn.LSTM\lreverseMap = {}\lgradOutput = {torch.CudaTensor[1x50x128]}" tooltip="[./LanguageModelSkipCon.lua]:58_"];
n8[label="Node8\ninput = {torch.CudaTensor[1x50]}\lmodule = nn.LookupTable\lreverseMap = {}\lgradOutput = {}" tooltip="[./LanguageModelSkipCon.lua]:41_"];
n9[label="Node9\ninput = {torch.CudaTensor[1x50]}\lreverseMap = {}\lgradOutput = {torch.CudaTensor[]}" tooltip="[[C]]:-1_"];
n1 -> n2;
n3 -> n1;
n4 -> n3;
n5 -> n4;
n6 -> n4;
n7 -> n5;
n7 -> n6;
n8 -> n7;
n9 -> n8;
n8[style=filled, fillcolor=red];
}